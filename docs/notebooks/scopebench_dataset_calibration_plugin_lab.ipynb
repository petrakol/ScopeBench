{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ScopeBench Dataset + Calibration + Plugin Lab\n",
        "\n",
        "This notebook is a hands-on continuation of the quickstart. It walks through:\n",
        "\n",
        "1. dataset case bootstrapping (`dataset-suggest`),\n",
        "2. validation and contribution checks,\n",
        "3. calibration tuning from telemetry, and\n",
        "4. plugin authoring and verification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Optional environment bootstrap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -U pip\n",
        "# %pip install -e \".[dev]\"\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def run_json(cmd: str):\n",
        "    raw = subprocess.check_output(cmd, shell=True, text=True)\n",
        "    return json.loads(raw)\n",
        "\n",
        "def run_text(cmd: str):\n",
        "    return subprocess.check_output(cmd, shell=True, text=True).strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Generate a draft dataset case with `dataset-suggest`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cmd = (\n",
        "    \"scopebench dataset-suggest \"\n",
        "    \"--id swe-notebook-001 \"\n",
        "    \"--domain swe \"\n",
        "    \"--instruction 'Stabilize flaky checkout tests without broad refactors' \"\n",
        "    \"--contract examples/coding_test_stabilization.contract.yaml \"\n",
        "    \"--plan examples/coding_test_stabilization.plan.yaml \"\n",
        "    \"--expected-decision ASK \"\n",
        "    \"--expected-rationale 'Needs stronger rollback and blast-radius controls' \"\n",
        "    \"--json\"\n",
        ")\n",
        "case = run_json(cmd)\n",
        "case.keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to append a generated case to a working dataset file.\n",
        "# run_text(\n",
        "#     \"scopebench dataset-suggest \"\n",
        "#     \"--id swe-notebook-001 \"\n",
        "#     \"--domain swe \"\n",
        "#     \"--instruction 'Stabilize flaky checkout tests without broad refactors' \"\n",
        "#     \"--contract examples/coding_test_stabilization.contract.yaml \"\n",
        "#     \"--plan examples/coding_test_stabilization.plan.yaml \"\n",
        "#     \"--expected-decision ASK \"\n",
        "#     \"--expected-rationale 'Needs stronger rollback and blast-radius controls' \"\n",
        "#     \"--append-to /tmp/community_cases.jsonl\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Validate dataset quality gates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace with your real file if needed.\n",
        "# validation = run_json(\"scopebench dataset-validate /tmp/community_cases.jsonl --json\")\n",
        "# validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Tune calibration from telemetry\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provide a real telemetry JSONL file to run this.\n",
        "# weekly = run_json(\"scopebench weekly-calibrate telemetry.jsonl --out axis_calibration.json --json\")\n",
        "# weekly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply saved calibration during evaluation.\n",
        "# calibrated = run_json(\n",
        "#     \"scopebench run examples/ops_rotate_key.contract.yaml examples/ops_rotate_key.plan.yaml \"\n",
        "#     \"--calibration-file axis_calibration.json --json\"\n",
        "# )\n",
        "# calibrated[\"decision\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Plugin authoring workflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive plugin generator\n",
        "# run_text(\"scopebench plugin-generate --out /tmp/robotics-starter.yaml\")\n",
        "\n",
        "# Lint + compatibility harness\n",
        "# run_text(\"scopebench plugin-lint /tmp/robotics-starter.yaml\")\n",
        "# run_text(\"scopebench plugin-harness /tmp/robotics-starter.yaml --max-golden-cases 100\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) API equivalents to automate onboarding\n",
        "\n",
        "If you are building internal onboarding portals, pair this notebook with:\n",
        "\n",
        "- `POST /dataset/suggest`\n",
        "- `POST /dataset/validate`\n",
        "- `GET /calibration/dashboard`\n",
        "- `POST /calibration/adjust`\n",
        "- `POST /plugins/wizard/generate`\n",
        "\n",
        "See the markdown tutorial: `docs/tutorials/dataset_calibration_plugin_lab.md`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}