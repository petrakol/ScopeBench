{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ScopeBench Interactive Quickstart Notebook\n",
        "\n",
        "This notebook walks through installation, template selection, plan editing, API usage, and Python integration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Environment setup\n",
        "Run this once in a fresh environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -U pip\n",
        "# %pip install -e \".[dev]\"\n",
        "import sys\n",
        "print(sys.version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Run CLI quickstarts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, subprocess\n",
        "\n",
        "def run_json(cmd: str):\n",
        "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
        "    try:\n",
        "        return json.loads(out)\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"raw\": out}\n",
        "\n",
        "quickstart = run_json(\"scopebench quickstart --json\")\n",
        "coding = run_json(\"scopebench coding-quickstart --json\")\n",
        "quickstart.get(\"decision\"), coding.get(\"decision\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Template selection\n",
        "Compare outcomes by changing `preset` values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile, textwrap\n",
        "\n",
        "plan_path = \"examples/coding_small.patch.plan.yaml\"\n",
        "\n",
        "def evaluate_with_preset(preset: str):\n",
        "    contract = textwrap.dedent(f'''\n",
        "    goal: \"Fix flaky test\"\n",
        "    preset: {preset}\n",
        "    ''').strip()\n",
        "    with tempfile.NamedTemporaryFile(\"w\", suffix=\".yaml\", delete=False) as f:\n",
        "        f.write(contract)\n",
        "        contract_path = f.name\n",
        "    out = subprocess.check_output(\n",
        "        f\"scopebench run {contract_path} {plan_path} --json\", shell=True, text=True\n",
        "    )\n",
        "    data = json.loads(out)\n",
        "    return {\"preset\": preset, \"decision\": data.get(\"decision\"), \"scores\": data.get(\"scores\", {})}\n",
        "\n",
        "[evaluate_with_preset(p) for p in [\"personal\", \"team\", \"enterprise\", \"regulated\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Plan editing sandbox\n",
        "Edit the plan text to make it more/less proportional and re-run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "plan_text = pathlib.Path(\"examples/phone_charge.plan.yaml\").read_text()\n",
        "print(plan_text[:600])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "edited_plan = plan_text.replace(\"deploy\", \"prototype\")  # example small scope reduction\n",
        "tmp_plan = tempfile.NamedTemporaryFile(\"w\", suffix=\".yaml\", delete=False)\n",
        "_ = tmp_plan.write(edited_plan)\n",
        "tmp_plan.close()\n",
        "\n",
        "result = subprocess.check_output(\n",
        "    f\"scopebench run examples/phone_charge.contract.yaml {tmp_plan.name} --json\",\n",
        "    shell=True,\n",
        "    text=True,\n",
        ")\n",
        "json.loads(result).get(\"decision\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) API usage (`/evaluate`)\n",
        "Make sure `scopebench serve --host 0.0.0.0 --port 8080` is running in another terminal first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "payload = {\n",
        "    \"contract\": {\"goal\": \"Fix failing unit test\", \"preset\": \"team\"},\n",
        "    \"plan\": {\n",
        "        \"task\": \"Fix failing unit test\",\n",
        "        \"steps\": [\n",
        "            {\"id\": \"1\", \"description\": \"Read failing test\", \"tool\": \"git_read\"},\n",
        "            {\"id\": \"2\", \"description\": \"Apply minimal patch\", \"tool\": \"git_patch\", \"depends_on\": [\"1\"]},\n",
        "            {\"id\": \"3\", \"description\": \"Run targeted test\", \"tool\": \"pytest\", \"depends_on\": [\"2\"]}\n",
        "        ]\n",
        "    },\n",
        "    \"include_summary\": True,\n",
        "    \"include_next_steps\": True,\n",
        "    \"include_patch\": True,\n",
        "    \"include_telemetry\": True\n",
        "}\n",
        "resp = requests.post(\"http://localhost:8080/evaluate\", json=payload, timeout=15)\n",
        "resp.status_code, resp.json().get(\"decision\"), resp.json().get(\"summary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Integration in Python code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scopebench.integrations.sdk import evaluate_plan\n",
        "\n",
        "integration_result = evaluate_plan(\n",
        "    contract={\"goal\": \"Fix failing unit test\", \"preset\": \"team\"},\n",
        "    plan={\n",
        "        \"task\": \"Fix failing unit test\",\n",
        "        \"steps\": [\n",
        "            {\"id\": \"1\", \"description\": \"Read failing test\", \"tool\": \"git_read\"},\n",
        "            {\"id\": \"2\", \"description\": \"Apply minimal patch\", \"tool\": \"git_patch\", \"depends_on\": [\"1\"]},\n",
        "            {\"id\": \"3\", \"description\": \"Run targeted test\", \"tool\": \"pytest\", \"depends_on\": [\"2\"]},\n",
        "        ]\n",
        "    },\n",
        "    include_summary=True,\n",
        ")\n",
        "integration_result.get(\"decision\"), integration_result.get(\"summary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Suggested experiments\n",
        "- Change presets and compare shifts from `ALLOW` to `ASK` or `DENY`.\n",
        "- Add higher-impact tools to test cumulative scope.\n",
        "- Compare direct SDK use vs. API mode in your own orchestrator.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}