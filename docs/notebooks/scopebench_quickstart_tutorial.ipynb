{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ScopeBench Interactive Quickstart Notebook\n",
        "\n",
        "A guided, hands-on tutorial for template selection, plan editing, effect annotation, and API evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Environment setup\n",
        "Run these once in a fresh environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# %pip install -U pip\n",
        "# %pip install -e \".[dev]\"\n",
        "import sys\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Template selection\n",
        "Start by discovering available templates and choosing a baseline example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json, subprocess\n",
        "\n",
        "def run_json(cmd: str):\n",
        "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
        "    return json.loads(out)\n",
        "\n",
        "print(subprocess.check_output(\"scopebench list-templates\", shell=True, text=True))\n",
        "baseline = run_json(\"scopebench run examples/coding_bugfix.contract.yaml examples/coding_bugfix.plan.yaml --json\")\n",
        "baseline[\"decision\"], baseline.get(\"scores\", {})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Plan editing sandbox\n",
        "Modify a plan and compare decisions before/after edits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "original_path = Path(\"examples/coding_bugfix.plan.yaml\")\n",
        "plan_text = original_path.read_text()\n",
        "print(plan_text[:700])\n",
        "\n",
        "# Example edit: narrow potentially broad wording\n",
        "edited_text = plan_text.replace(\"Apply broad refactor\", \"Apply minimal targeted refactor\")\n",
        "with tempfile.NamedTemporaryFile(\"w\", suffix=\".yaml\", delete=False) as f:\n",
        "    _ = f.write(edited_text)\n",
        "    edited_plan_path = f.name\n",
        "\n",
        "before = run_json(\"scopebench run examples/coding_bugfix.contract.yaml examples/coding_bugfix.plan.yaml --json\")\n",
        "after = run_json(f\"scopebench run examples/coding_bugfix.contract.yaml {edited_plan_path} --json\")\n",
        "{\n",
        "    \"before_decision\": before[\"decision\"],\n",
        "    \"after_decision\": after[\"decision\"],\n",
        "    \"before_reasons\": before.get(\"reasons\", []),\n",
        "    \"after_reasons\": after.get(\"reasons\", []),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Effect annotation\n",
        "Use ScopeBench effect suggestions to enrich plan metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "suggested = run_json(f\"scopebench suggest-effects {edited_plan_path} --json\")\n",
        "list(suggested.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To apply effect annotations directly to the plan file, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "subprocess.check_output(f\"scopebench suggest-effects {edited_plan_path} --in-place\", shell=True, text=True)\n",
        "post_effect = run_json(f\"scopebench run examples/coding_bugfix.contract.yaml {edited_plan_path} --json\")\n",
        "post_effect[\"decision\"], post_effect.get(\"scores\", {})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) API usage (`/evaluate`)\n",
        "Make sure `scopebench serve --host 0.0.0.0 --port 8080` is running in another terminal first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "payload = {\n",
        "    \"contract\": {\"goal\": \"Fix failing unit test\", \"preset\": \"team\"},\n",
        "    \"plan\": {\n",
        "        \"task\": \"Fix failing unit test\",\n",
        "        \"steps\": [\n",
        "            {\"id\": \"1\", \"description\": \"Read failing test\", \"tool\": \"git_read\"},\n",
        "            {\"id\": \"2\", \"description\": \"Apply minimal patch\", \"tool\": \"git_patch\", \"depends_on\": [\"1\"]},\n",
        "            {\"id\": \"3\", \"description\": \"Run targeted test\", \"tool\": \"pytest\", \"depends_on\": [\"2\"]},\n",
        "        ],\n",
        "    },\n",
        "    \"include_summary\": True,\n",
        "    \"include_next_steps\": True,\n",
        "    \"include_patch\": True,\n",
        "}\n",
        "\n",
        "r = requests.post(\"http://localhost:8080/evaluate\", json=payload, timeout=30)\n",
        "r.raise_for_status()\n",
        "result = r.json()\n",
        "result[\"decision\"], result.get(\"summary\"), result.get(\"next_steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Integration in Python code\n",
        "The SDK wrapper provides the same behavior directly in your pipelines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from scopebench.integrations.sdk import evaluate_plan\n",
        "\n",
        "integration_result = evaluate_plan(\n",
        "    contract={\"goal\": \"Fix failing unit test\", \"preset\": \"team\"},\n",
        "    plan={\n",
        "        \"task\": \"Fix failing unit test\",\n",
        "        \"steps\": [\n",
        "            {\"id\": \"1\", \"description\": \"Read failing test\", \"tool\": \"git_read\"},\n",
        "            {\"id\": \"2\", \"description\": \"Apply minimal patch\", \"tool\": \"git_patch\", \"depends_on\": [\"1\"]},\n",
        "            {\"id\": \"3\", \"description\": \"Run targeted test\", \"tool\": \"pytest\", \"depends_on\": [\"2\"]},\n",
        "        ],\n",
        "    },\n",
        "    include_summary=True,\n",
        ")\n",
        "integration_result[\"decision\"], integration_result.get(\"summary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Suggested follow-up experiments\n",
        "- Compare decisions across presets: `personal`, `team`, `enterprise`, `regulated`.\n",
        "- Add/remove steps and observe cumulative scoring effects.\n",
        "- Use API toggles like `include_telemetry` and `shadow_mode` for integration testing."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}